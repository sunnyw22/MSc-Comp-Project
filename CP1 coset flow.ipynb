{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant potential CNF appraoch for learning $\\mathbb{CP}^1$ model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  9 08:58:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  Tesla T4                       Off | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   42C    P0              33W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from functools import partial\n",
    "from typing import Callable\n",
    "import chex\n",
    "\n",
    "from coset_flow.glutils import sample_haar, sample_haar_lattice, SU2_GEN, liegrad, roll_lattice\n",
    "from coset_flow.jaxcg import CG3, crouch_grossmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement \n",
    "- Pairwise Function potential \n",
    "- The 'h Map' (Hopf Fibration) for mapping SU(2) matrices to CP^1 vectors (but keep the phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRNGSequence:\n",
    "    def __init__(self, rng):\n",
    "        self._rng = rng\n",
    "    def __next__(self):\n",
    "        self._rng, rng = jax.random.split(self._rng)\n",
    "        return rng\n",
    "\n",
    "rng = jax.random.PRNGKey(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the hopf map:\n",
    "$$SU(2) \\ni \\left( \\begin{matrix} z_{1} & -z_{2}^{*} \\\\ z_2 & z_{1}^{*} \\end{matrix} \\right) \\rightarrow \\left( \\begin{matrix} \\cos \\phi \\\\ -\\sin\\phi e^{-i(\\alpha + \\beta)} \\end{matrix}\\right) e^{i\\alpha} \\in \\mathbb{CP}^1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping on Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def su2_to_cp1(samples):\n",
    "    # Assumes input shape (2, 2)\n",
    "    z1 = samples[0,0]\n",
    "    z2 = samples[1,0]\n",
    "\n",
    "    # Extract U(1) phase\n",
    "    phase = jnp.exp(1j * jnp.angle(z1))\n",
    "    # Extract cos_phi\n",
    "    cos_phi = jnp.real(z1 / phase)  \n",
    "    # Construct the CP^1 vector without the phase\n",
    "    cp1_vector = jnp.array([cos_phi, z2 * jnp.conjugate(phase)])\n",
    "\n",
    "    return cp1_vector\n",
    "\n",
    "def cp1_to_su2(cp1_vec, phase):\n",
    "    # Assumes input shape cp1_vec: (Counts, 2), phase: (Counts, 1)\n",
    "    cp1_vec = cp1_vec * phase\n",
    "\n",
    "    z1 = cp1_vec[:, 0]\n",
    "    z2 = cp1_vec[:, 1]\n",
    "\n",
    "    # Construct the SU(2) matrices\n",
    "    su2_recon = jnp.array([\n",
    "        [z1, -jnp.conjugate(z2)],\n",
    "        [z2, jnp.conjugate(z1)]\n",
    "    ])\n",
    "\n",
    "    # Rearrange axes to get the correct shape (num_samples, 2, 2)\n",
    "    return jnp.transpose(su2_recon, axes=(2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def su2_to_cp1_lattice(lattice):\n",
    "    # Extract the elements z1 and z2 from the SU(2) matrices\n",
    "    z1 = lattice[..., 0, 0]  # Shape: (sample size, N, N)\n",
    "    z2 = lattice[..., 1, 0]  # Shape: (sample size, N, N)\n",
    "    \n",
    "    # Extract the U(1) phase\n",
    "    phase = jnp.exp(1j * jnp.angle(z1))  # Shape: (sample size, N, N)\n",
    "    \n",
    "    # Normalize z1 to extract cos_phi (by removing the phase)\n",
    "    cos_phi = jnp.real(z1 / phase)  # Shape: (sample size, N, N)\n",
    "    \n",
    "    # Construct the CP^1 vector without the phase\n",
    "    cp1_vector = jnp.stack([cos_phi, z2 * jnp.conjugate(phase)], axis=-1)  # Shape: (sample size, N, N, 2)\n",
    "    \n",
    "    # Reshape phase to match the output shape requirement\n",
    "    phase = phase[..., jnp.newaxis]  # Shape: (sample size, N, N, 1)\n",
    "\n",
    "    return cp1_vector, phase\n",
    "\n",
    "\n",
    "def cp1_to_su2_lattice(cp1_vec, phase):\n",
    "    # Multiply CP^1 vectors by their corresponding phases to recover the original SU(2) vectors\n",
    "    cp1_vec = cp1_vec * phase\n",
    "\n",
    "    z1 = cp1_vec[..., 0]  # Shape: (sample size, N, N)\n",
    "    z2 = cp1_vec[..., 1]  # Shape: (sample size, N, N)\n",
    "\n",
    "    # Construct the SU(2) matrices\n",
    "    su2_recon = jnp.stack([\n",
    "        jnp.stack([z1, -jnp.conjugate(z2)], axis=-1),  # First row of the SU(2) matrix\n",
    "        jnp.stack([z2, jnp.conjugate(z1)], axis=-1)    # Second row of the SU(2) matrix\n",
    "    ], axis=-2)  # Stack along the second-to-last axis to form the 2x2 matrix\n",
    "\n",
    "    # Final shape: (sample size, N, N, 2, 2)\n",
    "    return su2_recon        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_local(nn.Module):\n",
    "    hidden_features: int = 128\n",
    "    num_layers: int = 30\n",
    "    activation: Callable = jax.nn.tanh\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, t, x):\n",
    "        # Append time dimension to the input\n",
    "        t_p = jnp.expand_dims(t, axis=-1)\n",
    "        t_p = jnp.broadcast_to(t_p, x.shape[:-1] + (1,))\n",
    "        x = jnp.concatenate([x, t_p], axis=-1)\n",
    "        x = nn.Dense(10, kernel_init=nn.initializers.xavier_normal())(x)\n",
    "\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nn.Dense(self.hidden_features, kernel_init=nn.initializers.xavier_normal())(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = nn.Dense(1, kernel_init=nn.initializers.xavier_normal())(x)  \n",
    "        return x.reshape(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(U):\n",
    "    \"\"\"Get neighboring SU(2) matrices for each point in the lattice.\"\"\"\n",
    "    # Input shape (N, N, 2, 2)\n",
    "    U_up = roll_lattice(U, (1, 0), invert=False) \n",
    "    U_down = roll_lattice(U, (1, 0), invert=True)  \n",
    "    U_left = roll_lattice(U, (0, 1), invert=False)  \n",
    "    U_right = roll_lattice(U, (0, 1), invert=True)  \n",
    "    return U_up, U_down, U_left, U_right\n",
    "\n",
    "def pairwise_products(cp1, cp1_up, cp1_down, cp1_left, cp1_right):\n",
    "    \"\"\"Performs pairwise products at a single lattice site\"\"\"\n",
    "\n",
    "    dot1 = jnp.vdot(cp1, cp1)        # This is always 1, perhaps can be removed\n",
    "    dot2 = jnp.vdot(cp1, cp1_up)\n",
    "    dot3 = jnp.vdot(cp1, cp1_down)\n",
    "    dot4 = jnp.vdot(cp1, cp1_left)\n",
    "    dot5 = jnp.vdot(cp1, cp1_right)\n",
    "\n",
    "    products = jnp.array([\n",
    "        dot1.real, dot1.imag,\n",
    "        dot2.real, dot2.imag,\n",
    "        dot3.real, dot3.imag,\n",
    "        dot4.real, dot4.imag,\n",
    "        dot5.real, dot5.imag,\n",
    "    ])\n",
    "    \n",
    "    return products\n",
    "\n",
    "class InvPotential(nn.Module):\n",
    "    N: int\n",
    "    hidden_features: int = 64\n",
    "\n",
    "    def setup(self):\n",
    "        self.MLP = MLP_local(hidden_features=self.hidden_features)\n",
    "\n",
    "    def __call__(self, t, U_up, U_down, U_left, U_right, U):\n",
    "\n",
    "        cp1 = su2_to_cp1(U)\n",
    "        cp1_up = su2_to_cp1(U_up)\n",
    "        cp1_down = su2_to_cp1(U_down)\n",
    "        cp1_left = su2_to_cp1(U_left)\n",
    "        cp1_right = su2_to_cp1(U_right)\n",
    "\n",
    "        x = pairwise_products(cp1, cp1_up, cp1_down, cp1_left, cp1_right)\n",
    "\n",
    "        return self.MLP(t, x)\n",
    "    \n",
    "class Equiv_VF(nn.Module):\n",
    "    N: int\n",
    "    hidden_features: int = 128\n",
    "\n",
    "    def setup(self):\n",
    "        self.potential = InvPotential(N=self.N, hidden_features=self.hidden_features)\n",
    "\n",
    "    def vector_field_ij(self, params, t, U_up, U_down, U_left, U_right, U):\n",
    "        grad_potential = liegrad.grad(self.potential.apply, argnum=2, algebra=SU2_GEN)\n",
    "        return grad_potential(params, t, U_up, U_down, U_left, U_right, U)\n",
    "\n",
    "    def __call__(self, params, t, U):\n",
    "\n",
    "        U_up, U_down, U_left, U_right = get_neighbors(U)\n",
    "\n",
    "        vector_field = jax.vmap(jax.vmap(self.vector_field_ij, \n",
    "                            in_axes=(None, None, 0, 0, 0, 0, 0)),\n",
    "                                in_axes=(None, None, 0, 0, 0, 0, 0))\n",
    "\n",
    "        return vector_field(params, t, U_up, U_down, U_left, U_right, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "    N: int\n",
    "    t0: float\n",
    "    t1: float\n",
    "    hidden_features: int = 64\n",
    "    steps: int = 100\n",
    "    int_step = 10\n",
    "\n",
    "    def setup(self):\n",
    "        self.potential = InvPotential(N=self.N, hidden_features=self.hidden_features)\n",
    "\n",
    "    def forward(self, U, p_params):   \n",
    "\n",
    "        def val_grad_div(t, U, p_params):\n",
    "\n",
    "            def val_grad_div_single_sample(U):\n",
    "                U_up, U_down, U_left, U_right = get_neighbors(U)\n",
    "                # Calculate divergence using the potential which needs U and its neighbors\n",
    "                _, grad, div = liegrad.value_grad_divergence(partial(self.potential.apply, p_params, t, U_up, U_down, U_left, U_right), U, SU2_GEN)\n",
    "                return grad, div\n",
    "\n",
    "            # Vectorize over the entire lattice\n",
    "            grad, div = jax.vmap(jax.vmap(jax.vmap(val_grad_div_single_sample, in_axes=0), in_axes=0), in_axes=0)(U)\n",
    "            \n",
    "            return grad, -div.sum(axis=(1, 2)) \n",
    "\n",
    "        x = U\n",
    "        logp = 0.\n",
    "        for t in range(self.int_step):\n",
    "\n",
    "            grad0, _ = val_grad_div(t / self.int_step, x, p_params)\n",
    "\n",
    "            x = x + grad0\n",
    "            _, div = val_grad_div(t / self.int_step, x, p_params)\n",
    "\n",
    "            logp += div / self.int_step\n",
    "\n",
    "        return U, logp\n",
    "    \n",
    "    def __call__(self, U, p_params):\n",
    "        return self.forward(U, p_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\mathbb{CP}^1$ Action \n",
    "$$S = \\frac{1}{g}\\sum_{n,\\mu} (1 - \\left| \\bar{z}_{n+ae_{\\mu}} \\cdot z_n\\right|^2)$$\n",
    "where $a$ = lattice spacing, $e_\\mu$ is the unit vector in $\\mu$ direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def compute_action(cp1_lattice, g):\\n    \\n    shifts = jnp.array([[1, 0], [0, 1]])\\n\\n    shifted_lattices = jax.vmap(lambda shift: jnp.roll(cp1_lattice, shift=shift, axis=(0, 1)))(shifts)\\n    z_dot = jnp.einsum('dnij,nij->dnij', shifted_lattices, jnp.conj(cp1_lattice))\\n    norm_z_dot = jnp.abs(z_dot) ** 2\\n    \\n    action = jnp.sum(1 - norm_z_dot)\\n    \\n    return action / g\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_action(cp1_lattice, g):\n",
    "       \n",
    "    action = 0.0\n",
    "\n",
    "    # Iterate over the directions (mu_x = (1, 0) and mu_y = (0, 1))\n",
    "    for mu in [(1, 0), (0, 1)]:\n",
    "        # Shift cp1_lattice in the mu direction using periodic boundary conditions\n",
    "        shifted_lattice = jnp.roll(cp1_lattice, shift=mu, axis=(0, 1))\n",
    "        z_dot = jnp.sum(jnp.conj(cp1_lattice) * shifted_lattice, axis=-1)\n",
    "        norm_z_dot = jnp.abs(z_dot) ** 2\n",
    "        action += jnp.sum(1 - norm_z_dot, axis=(0, 1))\n",
    "\n",
    "    return action / g\n",
    "\n",
    "@chex.dataclass\n",
    "class CP1Theory:\n",
    "    \"\"\"CP^1 model theory.\"\"\"\n",
    "    shape: tuple[int, ...]  # Lattice shape (e.g., (N, N) for 2D lattice)\n",
    "    g: chex.Scalar          # Coupling constant\n",
    "\n",
    "    @property\n",
    "    def lattice_size(self):\n",
    "        return jnp.prod(jnp.array(self.shape))\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return len(self.shape)\n",
    "\n",
    "    def action(self, cp1_lattice: jnp.ndarray, *, g: chex.Scalar = None) -> jnp.ndarray:\n",
    "\n",
    "        g = self.g if g is None else g\n",
    "\n",
    "        # Determine if we're working with a batch or a single configuration\n",
    "        if cp1_lattice.ndim == self.dim + 1:\n",
    "            # Single configuration: shape (N, N, 2)\n",
    "            chex.assert_shape(cp1_lattice, self.shape + (2,))\n",
    "            action = compute_action(cp1_lattice, g)\n",
    "            return action\n",
    "        else:\n",
    "            # Batch of configurations: shape (batch_size, N, N, 2)\n",
    "            chex.assert_shape(cp1_lattice[0], self.shape + (2,))\n",
    "            act = partial(compute_action, g=g)\n",
    "            action = jax.vmap(act)(cp1_lattice)\n",
    "            return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions, Loss functions and ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def reverse_dkl(logp: jnp.ndarray, logq: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.mean(logq - logp)\n",
    "\n",
    "@jax.jit\n",
    "def effective_sample_size(logp: jnp.ndarray, logq: jnp.ndarray) -> jnp.ndarray:\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*jax.nn.logsumexp(logw, axis=0) - jax.nn.logsumexp(2*logw, axis=0)\n",
    "    ess_per_sample = jnp.exp(log_ess) / len(logw)\n",
    "    return ess_per_sample\n",
    "\n",
    "def moving_average(x: jnp.ndarray, window: int = 10):\n",
    "    if len(x) < window:\n",
    "        return jnp.mean(x, keepdims=True)\n",
    "    else:\n",
    "        return jnp.convolve(x, jnp.ones(window), 'valid') / window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_live_plot(figsize=(8, 4), logit_scale=True, **kwargs):\n",
    "    fig, ax_ess = plt.subplots(1, 1, figsize=figsize, **kwargs)\n",
    "\n",
    "    ess_line = plt.plot([0], [0.5], color='C0', label='ESS')\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('ESS')\n",
    "    if logit_scale:\n",
    "        ax_ess.set_yscale('logit')\n",
    "    else:\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "    ax_loss = ax_ess.twinx()\n",
    "    loss_line = plt.plot([0], [1], color='C1', label='KL Loss')\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    lines = ess_line + loss_line\n",
    "    plt.legend(lines, [line.get_label() for line in lines], loc='upper center', ncol=2)\n",
    "\n",
    "    setup = dict(\n",
    "        fig=fig, ax_ess=ax_ess, ax_loss=ax_loss,\n",
    "        ess_line=ess_line, loss_line=loss_line, logit=logit_scale)\n",
    "    \n",
    "    display_id = display(fig, display_id=True)\n",
    "    setup['display_id'] = display_id\n",
    "\n",
    "    return setup\n",
    "\n",
    "\n",
    "def update_plots(history, setup, window_size=15):\n",
    "    ess_line = setup['ess_line']\n",
    "    loss_line = setup['loss_line']\n",
    "    ax_loss = setup['ax_loss']\n",
    "    ax_ess = setup['ax_ess']\n",
    "    fig = setup['fig']\n",
    "\n",
    "    ess = np.array(history['ess'])\n",
    "    ess = moving_average(ess, window=window_size)\n",
    "    steps = np.arange(len(ess))\n",
    "    ess_line[0].set_ydata(ess)\n",
    "    ess_line[0].set_xdata(steps)\n",
    "    if setup['logit'] and len(ess) > 1:\n",
    "        ax_ess.relim()\n",
    "        ax_ess.autoscale_view()\n",
    "\n",
    "    loss = np.array(history['loss'])\n",
    "    loss = moving_average(loss, window=window_size)\n",
    "    loss_line[0].set_ydata(loss)\n",
    "    loss_line[0].set_xdata(steps)\n",
    "    if len(loss) > 1:\n",
    "        ax_loss.relim()\n",
    "        ax_loss.autoscale_view()\n",
    "\n",
    "    setup['display_id'].update(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice_shape = (6,6)\n",
    "batch_size = 128\n",
    "U_init = sample_haar_lattice(rng, 1, lattice_shape)[0]\n",
    "U_1, U_2, U_3, U_4 = get_neighbors(U_init)\n",
    "\n",
    "rng_seq = PRNGSequence(rng)\n",
    "rng = next(rng_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = lattice_shape[0]\n",
    "g = 2.5\n",
    "t0 = 0\n",
    "t1 = 1 \n",
    "hidden_features = 128\n",
    "\n",
    "potential = InvPotential(N=N, hidden_features=hidden_features)\n",
    "p_params = potential.init(rng, t0, U_1, U_2, U_3, U_4, U_init) \n",
    "# p_params = jax.tree.map(lambda x: x.astype(jnp.float64), p_params)\n",
    "\n",
    "rng_seq = PRNGSequence(rng)\n",
    "rng = next(rng_seq)\n",
    "\n",
    "model = CNF(N=N, t0=t0, t1=t1, hidden_features=hidden_features)\n",
    "model_params = {}\n",
    "\n",
    "theory = CP1Theory(shape=lattice_shape, g=g)\n",
    "\n",
    "def _loss_fn(p_params, rng):\n",
    "\n",
    "    su2_haar = sample_haar_lattice(rng, batch_size, lattice_shape)\n",
    "    # _, phases_lattice = su2_to_cp1_lattice(su2_haar)\n",
    "\n",
    "    U_t, logq = model.apply(model_params, su2_haar, p_params)\n",
    "    cp1_lattice, _ = su2_to_cp1_lattice(U_t)\n",
    "    logp = -theory.action(cp1_lattice)\n",
    "    \n",
    "    dkl = reverse_dkl(logp, logq)\n",
    "    return dkl, (logq, logp, cp1_lattice)\n",
    "\n",
    "# using jax, we can generate the gradient function\n",
    "value_and_grad = jax.value_and_grad(_loss_fn, has_aux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an optimizer\n",
    "lr = optax.exponential_decay(0.00005, 8000, 1e-1)\n",
    "opt = optax.adam(lr, .8, .9)\n",
    "opt_state = opt.init(p_params)\n",
    "\n",
    "# given parameters and optimizer state, do one update step\n",
    "@jax.jit\n",
    "def _update_step(rng, p_params, opt_state):\n",
    "    (loss, (logq, logp, cp1)), grad = value_and_grad(p_params, rng)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p_params = optax.apply_updates(p_params, updates)\n",
    "    return p_params, opt_state, loss, effective_sample_size(logp, logq), cp1\n",
    "\n",
    "# we will later keep lists of the loss and ESS history, which we update here\n",
    "def update_step(rng, p_params, opt_state, history):\n",
    "    p_params, opt_state, loss, ess, _ = _update_step(rng, p_params, opt_state)\n",
    "    history['loss'].append(loss)\n",
    "    history['ess'].append(ess)\n",
    "    return p_params, opt_state, ess, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "epochs = 50\n",
    "epoch_size = 10\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'ess': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plotting\n",
    "plot_config = init_live_plot()\n",
    "counter = 0\n",
    "for era in range(epochs):\n",
    "    for epoch in range(epoch_size):\n",
    "        counter += 1\n",
    "        p_params, opt_state, ess, loss = update_step(rng, p_params, opt_state, history)\n",
    "        rng_seq = PRNGSequence(rng)\n",
    "        rng = next(rng_seq)\n",
    "        print(f\"Epoch: {counter}, ESS:, {ess}, KL: {loss}\")\n",
    "    update_plots(history, plot_config)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
